{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc446d7d-265d-4eeb-ad7b-242459914164",
   "metadata": {},
   "source": [
    "# Лабораторная работа № 8\n",
    "\n",
    "Генерация текста на основе “Алисы в стране чудес”\n",
    "    \n",
    "Задачи:\n",
    "\n",
    "   1. Ознакомиться с генерацией текста\n",
    "   2. Ознакомиться с системой Callback в Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282579c0",
   "metadata": {},
   "source": [
    "# Цель работы:\n",
    "Рекуррентные нейронные сети также могут быть использованы в качестве генеративных\n",
    "моделей.\n",
    "Это означает, что в дополнение к тому, что они используются для прогнозных моделей\n",
    "(создания прогнозов), они могут изучать последовательности проблемы, а затем\n",
    "генерировать совершенно новые вероятные последовательности для проблемной\n",
    "области.\n",
    "Подобные генеративные модели полезны не только для изучения того, насколько хорошо\n",
    "модель выявила проблему, но и для того, чтобы узнать больше о самой проблемной\n",
    "области."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4394b0da-c0a2-45cf-837b-6c996fdee9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20acf963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144522\n",
      "Total Vocab:  48\n"
     ]
    }
   ],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "482dcda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  144422\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b8ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bb38468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.9700\n",
      "Epoch 1: loss improved from inf to 2.96996, saving model to weights-improvement-01-2.9700.hdf5\n",
      "1129/1129 [==============================] - 359s 317ms/step - loss: 2.9700\n",
      "Epoch 2/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.7709\n",
      "Epoch 2: loss improved from 2.96996 to 2.77092, saving model to weights-improvement-02-2.7709.hdf5\n",
      "1129/1129 [==============================] - 335s 297ms/step - loss: 2.7709\n",
      "Epoch 3/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.6661\n",
      "Epoch 3: loss improved from 2.77092 to 2.66609, saving model to weights-improvement-03-2.6661.hdf5\n",
      "1129/1129 [==============================] - 326s 289ms/step - loss: 2.6661\n",
      "Epoch 4/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5871\n",
      "Epoch 4: loss improved from 2.66609 to 2.58714, saving model to weights-improvement-04-2.5871.hdf5\n",
      "1129/1129 [==============================] - 312s 276ms/step - loss: 2.5871\n",
      "Epoch 5/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.5256\n",
      "Epoch 5: loss improved from 2.58714 to 2.52556, saving model to weights-improvement-05-2.5256.hdf5\n",
      "1129/1129 [==============================] - 312s 277ms/step - loss: 2.5256\n",
      "Epoch 6/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4707\n",
      "Epoch 6: loss improved from 2.52556 to 2.47070, saving model to weights-improvement-06-2.4707.hdf5\n",
      "1129/1129 [==============================] - 314s 278ms/step - loss: 2.4707\n",
      "Epoch 7/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.4174\n",
      "Epoch 7: loss improved from 2.47070 to 2.41745, saving model to weights-improvement-07-2.4174.hdf5\n",
      "1129/1129 [==============================] - 319s 283ms/step - loss: 2.4174\n",
      "Epoch 8/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3709\n",
      "Epoch 8: loss improved from 2.41745 to 2.37087, saving model to weights-improvement-08-2.3709.hdf5\n",
      "1129/1129 [==============================] - 320s 283ms/step - loss: 2.3709\n",
      "Epoch 9/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.3242\n",
      "Epoch 9: loss improved from 2.37087 to 2.32425, saving model to weights-improvement-09-2.3242.hdf5\n",
      "1129/1129 [==============================] - 320s 284ms/step - loss: 2.3242\n",
      "Epoch 10/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2813\n",
      "Epoch 10: loss improved from 2.32425 to 2.28130, saving model to weights-improvement-10-2.2813.hdf5\n",
      "1129/1129 [==============================] - 320s 283ms/step - loss: 2.2813\n",
      "Epoch 11/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2403\n",
      "Epoch 11: loss improved from 2.28130 to 2.24027, saving model to weights-improvement-11-2.2403.hdf5\n",
      "1129/1129 [==============================] - 322s 286ms/step - loss: 2.2403\n",
      "Epoch 12/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.2016\n",
      "Epoch 12: loss improved from 2.24027 to 2.20164, saving model to weights-improvement-12-2.2016.hdf5\n",
      "1129/1129 [==============================] - 316s 280ms/step - loss: 2.2016\n",
      "Epoch 13/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1634\n",
      "Epoch 13: loss improved from 2.20164 to 2.16342, saving model to weights-improvement-13-2.1634.hdf5\n",
      "1129/1129 [==============================] - 317s 280ms/step - loss: 2.1634\n",
      "Epoch 14/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.1277\n",
      "Epoch 14: loss improved from 2.16342 to 2.12770, saving model to weights-improvement-14-2.1277.hdf5\n",
      "1129/1129 [==============================] - 313s 277ms/step - loss: 2.1277\n",
      "Epoch 15/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0948\n",
      "Epoch 15: loss improved from 2.12770 to 2.09480, saving model to weights-improvement-15-2.0948.hdf5\n",
      "1129/1129 [==============================] - 320s 283ms/step - loss: 2.0948\n",
      "Epoch 16/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0628\n",
      "Epoch 16: loss improved from 2.09480 to 2.06284, saving model to weights-improvement-16-2.0628.hdf5\n",
      "1129/1129 [==============================] - 324s 287ms/step - loss: 2.0628\n",
      "Epoch 17/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0353\n",
      "Epoch 17: loss improved from 2.06284 to 2.03531, saving model to weights-improvement-17-2.0353.hdf5\n",
      "1129/1129 [==============================] - 318s 281ms/step - loss: 2.0353\n",
      "Epoch 18/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0055\n",
      "Epoch 18: loss improved from 2.03531 to 2.00550, saving model to weights-improvement-18-2.0055.hdf5\n",
      "1129/1129 [==============================] - 316s 280ms/step - loss: 2.0055\n",
      "Epoch 19/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9765\n",
      "Epoch 19: loss improved from 2.00550 to 1.97653, saving model to weights-improvement-19-1.9765.hdf5\n",
      "1129/1129 [==============================] - 315s 279ms/step - loss: 1.9765\n",
      "Epoch 20/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9512\n",
      "Epoch 20: loss improved from 1.97653 to 1.95122, saving model to weights-improvement-20-1.9512.hdf5\n",
      "1129/1129 [==============================] - 316s 280ms/step - loss: 1.9512\n",
      "Epoch 21/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9275\n",
      "Epoch 21: loss improved from 1.95122 to 1.92754, saving model to weights-improvement-21-1.9275.hdf5\n",
      "1129/1129 [==============================] - 1131s 1s/step - loss: 1.9275\n",
      "Epoch 22/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9055\n",
      "Epoch 22: loss improved from 1.92754 to 1.90551, saving model to weights-improvement-22-1.9055.hdf5\n",
      "1129/1129 [==============================] - 311s 275ms/step - loss: 1.9055\n",
      "Epoch 23/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8819\n",
      "Epoch 23: loss improved from 1.90551 to 1.88185, saving model to weights-improvement-23-1.8819.hdf5\n",
      "1129/1129 [==============================] - 314s 278ms/step - loss: 1.8819\n",
      "Epoch 24/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8606\n",
      "Epoch 24: loss improved from 1.88185 to 1.86062, saving model to weights-improvement-24-1.8606.hdf5\n",
      "1129/1129 [==============================] - 317s 281ms/step - loss: 1.8606\n",
      "Epoch 25/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8441\n",
      "Epoch 25: loss improved from 1.86062 to 1.84415, saving model to weights-improvement-25-1.8441.hdf5\n",
      "1129/1129 [==============================] - 310s 275ms/step - loss: 1.8441\n",
      "Epoch 26/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8246\n",
      "Epoch 26: loss improved from 1.84415 to 1.82460, saving model to weights-improvement-26-1.8246.hdf5\n",
      "1129/1129 [==============================] - 321s 284ms/step - loss: 1.8246\n",
      "Epoch 27/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8088\n",
      "Epoch 27: loss improved from 1.82460 to 1.80882, saving model to weights-improvement-27-1.8088.hdf5\n",
      "1129/1129 [==============================] - 307s 272ms/step - loss: 1.8088\n",
      "Epoch 28/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7938\n",
      "Epoch 28: loss improved from 1.80882 to 1.79375, saving model to weights-improvement-28-1.7938.hdf5\n",
      "1129/1129 [==============================] - 315s 279ms/step - loss: 1.7938\n",
      "Epoch 29/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8978\n",
      "Epoch 29: loss did not improve from 1.79375\n",
      "1129/1129 [==============================] - 309s 274ms/step - loss: 1.8978\n",
      "Epoch 30/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 2.0081\n",
      "Epoch 30: loss did not improve from 1.79375\n",
      "1129/1129 [==============================] - 311s 276ms/step - loss: 2.0081\n",
      "Epoch 31/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.9446\n",
      "Epoch 31: loss did not improve from 1.79375\n",
      "1129/1129 [==============================] - 321s 284ms/step - loss: 1.9446\n",
      "Epoch 32/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8722\n",
      "Epoch 32: loss did not improve from 1.79375\n",
      "1129/1129 [==============================] - 321s 284ms/step - loss: 1.8722\n",
      "Epoch 33/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.8215\n",
      "Epoch 33: loss did not improve from 1.79375\n",
      "1129/1129 [==============================] - 315s 279ms/step - loss: 1.8215\n",
      "Epoch 34/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7792\n",
      "Epoch 34: loss improved from 1.79375 to 1.77924, saving model to weights-improvement-34-1.7792.hdf5\n",
      "1129/1129 [==============================] - 316s 280ms/step - loss: 1.7792\n",
      "Epoch 35/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7486\n",
      "Epoch 35: loss improved from 1.77924 to 1.74864, saving model to weights-improvement-35-1.7486.hdf5\n",
      "1129/1129 [==============================] - 321s 284ms/step - loss: 1.7486\n",
      "Epoch 36/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7274\n",
      "Epoch 36: loss improved from 1.74864 to 1.72739, saving model to weights-improvement-36-1.7274.hdf5\n",
      "1129/1129 [==============================] - 320s 283ms/step - loss: 1.7274\n",
      "Epoch 37/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7191\n",
      "Epoch 37: loss improved from 1.72739 to 1.71908, saving model to weights-improvement-37-1.7191.hdf5\n",
      "1129/1129 [==============================] - 318s 282ms/step - loss: 1.7191\n",
      "Epoch 38/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7172\n",
      "Epoch 38: loss improved from 1.71908 to 1.71719, saving model to weights-improvement-38-1.7172.hdf5\n",
      "1129/1129 [==============================] - 322s 285ms/step - loss: 1.7172\n",
      "Epoch 39/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7143\n",
      "Epoch 39: loss improved from 1.71719 to 1.71432, saving model to weights-improvement-39-1.7143.hdf5\n",
      "1129/1129 [==============================] - 357s 316ms/step - loss: 1.7143\n",
      "Epoch 40/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7132\n",
      "Epoch 40: loss improved from 1.71432 to 1.71322, saving model to weights-improvement-40-1.7132.hdf5\n",
      "1129/1129 [==============================] - 1833s 2s/step - loss: 1.7132\n",
      "Epoch 41/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7153\n",
      "Epoch 41: loss did not improve from 1.71322\n",
      "1129/1129 [==============================] - 332s 294ms/step - loss: 1.7153\n",
      "Epoch 42/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7135\n",
      "Epoch 42: loss did not improve from 1.71322\n",
      "1129/1129 [==============================] - 320s 283ms/step - loss: 1.7135\n",
      "Epoch 43/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.7061\n",
      "Epoch 43: loss improved from 1.71322 to 1.70606, saving model to weights-improvement-43-1.7061.hdf5\n",
      "1129/1129 [==============================] - 382s 339ms/step - loss: 1.7061\n",
      "Epoch 44/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6971\n",
      "Epoch 44: loss improved from 1.70606 to 1.69706, saving model to weights-improvement-44-1.6971.hdf5\n",
      "1129/1129 [==============================] - 365s 324ms/step - loss: 1.6971\n",
      "Epoch 45/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6878\n",
      "Epoch 45: loss improved from 1.69706 to 1.68784, saving model to weights-improvement-45-1.6878.hdf5\n",
      "1129/1129 [==============================] - 372s 329ms/step - loss: 1.6878\n",
      "Epoch 46/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6771\n",
      "Epoch 46: loss improved from 1.68784 to 1.67706, saving model to weights-improvement-46-1.6771.hdf5\n",
      "1129/1129 [==============================] - 341s 302ms/step - loss: 1.6771\n",
      "Epoch 47/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6674\n",
      "Epoch 47: loss improved from 1.67706 to 1.66741, saving model to weights-improvement-47-1.6674.hdf5\n",
      "1129/1129 [==============================] - 326s 289ms/step - loss: 1.6674\n",
      "Epoch 48/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6627\n",
      "Epoch 48: loss improved from 1.66741 to 1.66272, saving model to weights-improvement-48-1.6627.hdf5\n",
      "1129/1129 [==============================] - 339s 300ms/step - loss: 1.6627\n",
      "Epoch 49/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6508\n",
      "Epoch 49: loss improved from 1.66272 to 1.65076, saving model to weights-improvement-49-1.6508.hdf5\n",
      "1129/1129 [==============================] - 339s 300ms/step - loss: 1.6508\n",
      "Epoch 50/50\n",
      "1129/1129 [==============================] - ETA: 0s - loss: 1.6400\n",
      "Epoch 50: loss improved from 1.65076 to 1.64001, saving model to weights-improvement-50-1.6400.hdf5\n",
      "1129/1129 [==============================] - 342s 303ms/step - loss: 1.6400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d83ab75f40>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss',\n",
    "verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "model.fit(X, y, epochs=50, batch_size=128,\n",
    "callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1e1698d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" aced\n",
      "along the course, here and there. there was no 'one, two, three, and\n",
      "away,' but they began runn \"\n",
      "ing than the wan sutteig of the thaless on the was of the wasee ou a thrne tael to the whnt satelk, and she white rabbit war she fadl as the could, and salde lutten and muoked an toe to tee har in a lorg,eroglt, and she tas aownog to then so her  and then sas no hor ane thing it an once as the caue pf the dour of the was anl the wisl on tit oooe, and the tam to aenen in a lorent to be thry sile she was oote to sooe to sae it sas aalen hirtens, and the was sot the lopk thit hir hert to soo  she was the ras an ohc cineer oe the wasee oh the was a little bro oftilg that saed-toe oadd ant anoog thit sith ot hadd oo tiet so hav eore to the was a wiry sidel so eand oo the bool  sha was a little so two the rame ti her head to sene the rage ti head thitg whsh she had aali badlln the tiite she had oo havd the horpe oo tee thong the hadd afdin, \n",
      "'thet sere th tee,  said the konk turtle as an oxpooning\n",
      "'\n",
      "alice was sot they lote and toeee agiin, and she whst hadd totnd her hnen thin she had aneng \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-50-1.6400.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "\n",
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50d456fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed:\n",
      "\" fully one can't hear\n",
      "oneself speak--and they don't seem to have any rules in particular;\n",
      "at least, i \"\n",
      "n a lortter of the kande of the saalit hn a lort ereat soink alout thme the radle  the rase that io the tas toink it oo a bor,  the had not the kant was sot the ladd of the dourt, and then sas anilt hord that she was not it tomh oo she shoe of the court, whe was not a boo of the sabli  she lase whit was to tary toted of the sablit on tee thale, and the was sotting on the taale  the dart was so as the lodst ruote to salk the whine whsh she sable  the nast wiihe io she sade toinl in was oo aalin hnr fonn that sae in tae tadd to seel the had bavi\n",
      "task theer hor hem: and she was not al once an inde on the coore, and the was sutteig of the shaetesseone oe the white  an to aod thin ii the was oarking to teet to har and  'yhat as in so moooe,  she said to herself, and said an a lortee to cenned the sintese \n",
      "the was to tary tone, bnd saede to herself  tou know, \n",
      "\n",
      "'i dan t leke the hare,' she match hare said to alice, 'nhe sesh the duyrhos and teer to see tha had -adit  aller wat it titer as th\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# pick a random seed\n",
    "start = numpy.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print (\"Seed:\")\n",
    "print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "\n",
    "print (\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e8c7c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144522\n",
      "Total Vocab:  48\n",
      "Total Patterns:  144422\n",
      "WARNING:tensorflow:`batch_size` is no longer needed in the `TensorBoard` Callback and will be ignored in TensorFlow 2.0.\n",
      "Epoch 1/10\n",
      "283/283 [==============================] - 372s 1s/step - loss: 3.0656\n",
      "Epoch 2/10\n",
      "283/283 [==============================] - 380s 1s/step - loss: 2.9567\n",
      "Epoch 3/10\n",
      "283/283 [==============================] - 375s 1s/step - loss: 2.8470\n",
      "Epoch 4/10\n",
      "283/283 [==============================] - 392s 1s/step - loss: 2.7800\n",
      "Epoch 5/10\n",
      "283/283 [==============================] - 405s 1s/step - loss: 2.7313\n",
      "Epoch 6/10\n",
      "283/283 [==============================] - 396s 1s/step - loss: 2.6893\n",
      "Epoch 7/10\n",
      "283/283 [==============================] - 402s 1s/step - loss: 2.6418\n",
      "Epoch 8/10\n",
      "283/283 [==============================] - 429s 2s/step - loss: 2.6040\n",
      "Epoch 9/10\n",
      "283/283 [==============================] - 433s 2s/step - loss: 2.5685\n",
      "Epoch 10/10\n",
      "283/283 [==============================] - 560s 2s/step - loss: 2.5358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d84fc35400>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')\n",
    "\n",
    "# define the checkpoint\n",
    "tb_callback = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=0, batch_size=32,\n",
    "                                             write_graph=True, write_grads=False, write_images=False,\n",
    "                                             embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None,\n",
    "                                             embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "model.fit(X, y, epochs=10, batch_size=512, callbacks=[tb_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50579eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9b09299b7bb94a5d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9b09299b7bb94a5d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load the network weights\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "69348a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  144522\n",
      "Total Vocab:  48\n",
      "Total Patterns:  144422\n",
      "Epoch 1/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 3.0622Seed:\n",
      "\" \n",
      "the table was a large one, but the three were all crowded together at\n",
      "one corner of it: 'no room! n \"\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
      "Done.\n",
      "283/283 [==============================] - 374s 1s/step - loss: 3.0622\n",
      "Epoch 2/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.9584Seed:\n",
      "\" ing exclaimed, turning to the jury, who instantly made a\n",
      "memorandum of the fact.\n",
      "\n",
      "'i keep them to se \"\n",
      " th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th th \n",
      "Done.\n",
      "283/283 [==============================] - 378s 1s/step - loss: 2.9584\n",
      "Epoch 3/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.8506Seed:\n",
      "\" and, as the hall was very hot, she\n",
      "kept fanning herself all the time she went on talking: 'dear, dea \"\n",
      " toe  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa tee  aa\n",
      "Done.\n",
      "283/283 [==============================] - 371s 1s/step - loss: 2.8506\n",
      "Epoch 4/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.7899Seed:\n",
      "\" and your jaws are too weak\n",
      "    for anything tougher than suet;\n",
      "   yet you finished the goose, with t \"\n",
      "he  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad the  aad th\n",
      "Done.\n",
      "283/283 [==============================] - 371s 1s/step - loss: 2.7899\n",
      "Epoch 5/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.7421Seed:\n",
      "\" t of the\n",
      "wood--(she considered him to be a footman because he was in livery:\n",
      "otherwise, judging by h \"\n",
      "h the  and the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca the  an ca\n",
      "Done.\n",
      "283/283 [==============================] - 368s 1s/step - loss: 2.7421\n",
      "Epoch 6/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.6964Seed:\n",
      "\" --as it is, i can't get out at the door--i do wish i hadn't\n",
      "drunk quite so much!'\n",
      "\n",
      "alas! it was too  \"\n",
      "toe  and the  and toe toet toe  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and the  and t\n",
      "Done.\n",
      "283/283 [==============================] - 375s 1s/step - loss: 2.6964\n",
      "Epoch 7/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.6464Seed:\n",
      "\" , and one foot up the chimney, and said to herself 'now i\n",
      "can do no more, whatever happens. what wil \"\n",
      " woe toet to the toet  \n",
      "\n",
      "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
      "Done.\n",
      "283/283 [==============================] - 379s 1s/step - loss: 2.6464\n",
      "Epoch 8/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.6021Seed:\n",
      "\" ng about at the other end of the\n",
      "ground--and i should have croqueted the queen's hedgehog just now,  \"\n",
      "\n",
      "\n",
      "'                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "Done.\n",
      "283/283 [==============================] - 374s 1s/step - loss: 2.6021\n",
      "Epoch 9/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.5668Seed:\n",
      "\" f beautiful soup?\n",
      "     beau--ootiful soo--oop!\n",
      "     beau--ootiful soo--oop!\n",
      "   soo--oop of the e--e- \"\n",
      "te the                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
      "Done.\n",
      "283/283 [==============================] - 382s 1s/step - loss: 2.5668\n",
      "Epoch 10/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.5355Seed:\n",
      "\" it, 'and what is the use of a book,' thought alice 'without pictures or\n",
      "conversations?'\n",
      "\n",
      "so she was  \"\n",
      "ho the catter aadin  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet  and the woet \n",
      "Done.\n",
      "283/283 [==============================] - 387s 1s/step - loss: 2.5355\n",
      "Epoch 11/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.5050Seed:\n",
      "\" he\n",
      "conversation dropped, and the party sat silent for a minute, while alice\n",
      "thought over all she cou \"\n",
      " ho the woeee  and she soet hare an an an aale to the soeee  \n",
      "\n",
      "''                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
      "Done.\n",
      "283/283 [==============================] - 383s 1s/step - loss: 2.5050\n",
      "Epoch 12/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.4762Seed:\n",
      "\"  alice had no very clear notion how long ago\n",
      "anything had happened.) so she began again: 'ou est ma  \"\n",
      "toue toe tooe to the tooe to the toete  \n",
      "\n",
      "'i aon't toe toet ' said the monee \n",
      "'nh tou toen toe tooe                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
      "Done.\n",
      "283/283 [==============================] - 381s 1s/step - loss: 2.4762\n",
      "Epoch 13/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.4500Seed:\n",
      "\"  so that by\n",
      "the end of half an hour or so there were no arches left, and all the\n",
      "players, except the \"\n",
      " was to the toeee  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was toet ias  and the was t\n",
      "Done.\n",
      "283/283 [==============================] - 379s 1s/step - loss: 2.4500\n",
      "Epoch 14/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.4211Seed:\n",
      "\" ything seemed to have changed since her swim in\n",
      "the pool, and the great hall, with the glass table a \"\n",
      "adin  she was a lirtle woele  and the woele was an an a lett of the was  \n",
      "\n",
      "''                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "Done.\n",
      "283/283 [==============================] - 373s 1s/step - loss: 2.4211\n",
      "Epoch 15/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3968Seed:\n",
      "\"  the mock turtle sang this, very slowly\n",
      "and sadly:--\n",
      "\n",
      " '\"will you walk a little faster?\" said a whit \"\n",
      " on the wooe. \n",
      "'ne tou d sere the souee   shi kact taid to herself, 'nhe soued to the tooee oo the woue   \n",
      "''        *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *    *  \n",
      "Done.\n",
      "283/283 [==============================] - 383s 1s/step - loss: 2.3968\n",
      "Epoch 16/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3704Seed:\n",
      "\"  quite dry again,\n",
      "the dodo suddenly called out 'the race is over!' and they all crowded\n",
      "round it, pa \"\n",
      " the lore th the wast oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was oo the was\n",
      "Done.\n",
      "283/283 [==============================] - 397s 1s/step - loss: 2.3704\n",
      "Epoch 17/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3421Seed:\n",
      "\" ence!' and read out from his book, 'rule\n",
      "forty-two. all persons more than a mile high to leave the c \"\n",
      "ade  \n",
      "she said to herself, 'nh sou de ano the sooee ' she said to herself, ''whet wou dan toe cade to the sooee ' she gait raid to herself, ''whet wou dan toe cade to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe dare to the sooee ' she said to herself, ''whet wou dan toe d\n",
      "Done.\n",
      "283/283 [==============================] - 403s 1s/step - loss: 2.3421\n",
      "Epoch 18/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.3190Seed:\n",
      "\" y,\n",
      "because i was a different person then.'\n",
      "\n",
      "'explain all that,' said the mock turtle.\n",
      "\n",
      "'no, no! the  \"\n",
      "'at  shu ' said the monee  ''ne tou te tee   she mact toitee aegan in a lort oi thete \n",
      "\n",
      "'ie you dan t teee the catee'  shi mant rnierill an an anl aeren  \n",
      " '                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
      "Done.\n",
      "283/283 [==============================] - 431s 2s/step - loss: 2.3190\n",
      "Epoch 19/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.2925Seed:\n",
      "\" ad: it just grazed his nose,\n",
      "and broke to pieces against one of the trees behind him.\n",
      "\n",
      "'--or next da \"\n",
      "ve toued to tee 'the marter wo tee soene ' \n",
      "'i whal thu wound ' said the monee  'thu soued to tee the morer of the rooer '\n",
      "\n",
      "'i mone to toe tound ' said the monee  'thu soued to tee the couse    ''                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
      "Done.\n",
      "283/283 [==============================] - 443s 2s/step - loss: 2.2925\n",
      "Epoch 20/20\n",
      "283/283 [==============================] - ETA: 0s - loss: 2.2694Seed:\n",
      "\"  sharp hiss made\n",
      "her draw back in a hurry: a large pigeon had flown into her face, and\n",
      "was beating h \"\n",
      "o a lort of thete whet soeee aod the woede oo the coure, and then the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh, and when the was aol the woole oo the courh\n",
      "Done.\n",
      "283/283 [==============================] - 463s 2s/step - loss: 2.2694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d83c46a400>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"wonderland.txt\"\n",
    "raw_text = open(filename).read()\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print (\"Total Characters: \", n_chars)\n",
    "print (\"Total Vocab: \", n_vocab)\n",
    "\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print (\"Total Patterns: \", n_patterns)\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "optimizer='adam')\n",
    "\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "            if (epoch + 1)%1 == 0:\n",
    "                # pick a random seed\n",
    "                start = numpy.random.randint(0, len(dataX)-1)\n",
    "                pattern = dataX[start]\n",
    "                print (\"Seed:\")\n",
    "                print (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "    \n",
    "            # generate characters\n",
    "                for i in range(1000):\n",
    "                    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "                    x = x / float(n_vocab)\n",
    "                    prediction = model.predict(x, verbose=0) \n",
    "                    index = numpy.argmax(prediction)\n",
    "                    result = int_to_char[index]\n",
    "                    seq_in = [int_to_char[value] for value in pattern]\n",
    "                    sys.stdout.write(result)\n",
    "                    pattern.append(index)\n",
    "                    pattern = pattern[1:len(pattern)]\n",
    "                    \n",
    "                print (\"\\nDone.\")\n",
    "                \n",
    "# define the checkpoint\n",
    "#tb_callback = keras.callbacks.Callback.CustomCallback (log_dir='./logs', histogram_freq=0, batch_size=32,\n",
    "#write_graph=True, write_grads=False, write_images=False,\n",
    "#embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None,\n",
    "#embeddings_data=None, update_freq='epoch')\n",
    "\n",
    "model.fit(X, y, epochs=20, batch_size=512, callbacks=[CustomCallback()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fa4846",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
