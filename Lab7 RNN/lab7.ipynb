{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "643ee474-87fe-418c-91eb-fba8d5b4cee8",
   "metadata": {},
   "source": [
    "## Лабораторная работа No 7\n",
    "### Классификация обзоров фильмов\n",
    "## Цель\n",
    "Классификация последовательностей - это проблема прогнозирующего моделирования, когда у вас есть некоторая последовательность входных данных в пространстве или времени, и задача состоит в том, чтобы предсказать категорию для последовательности. Проблема усложняется тем, что последовательности могут различаться по длине, состоять из очень большого словарного запаса входных символов и могут потребовать от модели изучения долгосрочного контекста или зависимостей между символами во входной последовательности. В данной лабораторной работе также будет использоваться датасет IMDb, однако обучение будет проводиться с помощью рекуррентной нейронной сети.\n",
    "\n",
    "## Задачи\n",
    "1. Ознакомиться с рекуррентными нейронными сетями\n",
    "2. Изучить способы классификации текста\n",
    "3. Ознакомиться с ансамблированием сетей\n",
    "4. Построить ансамбль сетей, который позволит получать точность не менее 97%\n",
    "\n",
    "# Требования\n",
    "1. Найти набор оптимальных ИНС для классификации текста\n",
    "2. Провести ансамблирование моделей\n",
    "3. Написать функцию/функции, которые позволят загружать текст и получать результат ансамбля сетей\n",
    "4. Провести тестирование сетей на своих текстах (привести в отчете)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd70ee-d83b-4eb9-ae95-70e470419cf9",
   "metadata": {},
   "source": [
    "# Выполнение работы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e62a1a1-c298-412b-b32e-9f7bf6a738f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import numpy as np\n",
    "from tkinter import *\n",
    "from tkinter import ttk\n",
    "import tkinter as tk\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from keras.layers import MaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3776971c-e3a7-4dba-ba93-36f16cfe0e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 228s 580ms/step - loss: 0.4766 - accuracy: 0.7629 - val_loss: 0.3594 - val_accuracy: 0.8463\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 215s 551ms/step - loss: 0.3157 - accuracy: 0.8677 - val_loss: 0.3908 - val_accuracy: 0.8453\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 234s 599ms/step - loss: 0.2603 - accuracy: 0.8973 - val_loss: 0.2967 - val_accuracy: 0.8754\n",
      "Accuracy: 87.54%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=5000)\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(5000, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da2b2e92-28c3-4c0a-91ab-a629066dc45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 250, 32)          0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 216,405\n",
      "Trainable params: 216,405\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/3\n",
      "391/391 [==============================] - 116s 292ms/step - loss: 0.4467 - accuracy: 0.7698 - val_loss: 0.2912 - val_accuracy: 0.8792\n",
      "Epoch 2/3\n",
      "391/391 [==============================] - 110s 282ms/step - loss: 0.2456 - accuracy: 0.9042 - val_loss: 0.3132 - val_accuracy: 0.8719\n",
      "Epoch 3/3\n",
      "391/391 [==============================] - 109s 278ms/step - loss: 0.2029 - accuracy: 0.9224 - val_loss: 0.2959 - val_accuracy: 0.8838\n",
      "Accuracy: 88.38%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),\n",
    "axis=0)\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length,\n",
    "input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) #filters - Количество выходных фильтров в свертке\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
    "epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092a8306-7e25-42f9-9075-d90e3c7bccf0",
   "metadata": {},
   "source": [
    "## Применение слоёв Dropout в архитектуре нейронной сети"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b81a3a9-31ea-4f82-99ec-63b2813551d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500, 32)           0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " conv1d_2 (Conv1D)           (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 500, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d_1 (MaxPooling  (None, 250, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 250, 32)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 219,509\n",
      "Trainable params: 219,509\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/4\n",
      "782/782 [==============================] - 142s 179ms/step - loss: 0.4846 - accuracy: 0.7469 - val_loss: 0.3634 - val_accuracy: 0.8448\n",
      "Epoch 2/4\n",
      "782/782 [==============================] - 126s 161ms/step - loss: 0.2801 - accuracy: 0.8877 - val_loss: 0.2814 - val_accuracy: 0.8837\n",
      "Epoch 3/4\n",
      "782/782 [==============================] - 128s 164ms/step - loss: 0.2272 - accuracy: 0.9137 - val_loss: 0.2749 - val_accuracy: 0.8866\n",
      "Epoch 4/4\n",
      "782/782 [==============================] - 132s 168ms/step - loss: 0.2056 - accuracy: 0.9222 - val_loss: 0.2936 - val_accuracy: 0.8858\n",
      "Accuracy: 88.58%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),\n",
    "axis=0)\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) #filters - Количество выходных фильтров в свертке\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(LSTM(100))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=4, batch_size=32)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f63bedd0-fafa-4045-83fe-624dca0d11af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 500, 32)           160000    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 500, 32)           0         \n",
      "                                                                 \n",
      " conv1d_3 (Conv1D)           (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 500, 32)           3104      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 500, 32)           0         \n",
      "                                                                 \n",
      " max_pooling1d_2 (MaxPooling  (None, 250, 32)          0         \n",
      " 1D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 250, 32)           0         \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               53200     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 32)                3232      \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 222,673\n",
      "Trainable params: 222,673\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "782/782 [==============================] - 144s 182ms/step - loss: 0.5364 - accuracy: 0.7236 - val_loss: 0.5835 - val_accuracy: 0.7253\n",
      "Epoch 2/5\n",
      "782/782 [==============================] - 139s 178ms/step - loss: 0.3552 - accuracy: 0.8511 - val_loss: 0.3851 - val_accuracy: 0.8252\n",
      "Epoch 3/5\n",
      "782/782 [==============================] - 134s 171ms/step - loss: 0.2995 - accuracy: 0.8794 - val_loss: 0.4357 - val_accuracy: 0.8020\n",
      "Epoch 4/5\n",
      "782/782 [==============================] - 138s 177ms/step - loss: 0.2740 - accuracy: 0.8902 - val_loss: 0.3977 - val_accuracy: 0.8244\n",
      "Epoch 5/5\n",
      "782/782 [==============================] - 134s 171ms/step - loss: 0.2476 - accuracy: 0.9024 - val_loss: 0.6509 - val_accuracy: 0.7616\n",
      "Accuracy: 76.16%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "from keras.datasets import imdb\n",
    "(training_data, training_targets), (testing_data,\n",
    "testing_targets) = imdb.load_data(num_words=10000)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "\n",
    "targets = np.concatenate((training_targets, testing_targets),\n",
    "axis=0)\n",
    "\n",
    "top_words = 5000\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train,\n",
    "maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test,\n",
    "maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) #filters - Количество выходных фильтров в свертке\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(LSTM(100))\n",
    "model.add(keras.layers.Dropout(0.3, noise_shape=None, seed=None))\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=32)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a653fd6c-addb-474d-9915-fdf346e37cb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 455s 579ms/step - loss: 0.4007 - accuracy: 0.8169 - val_loss: 0.1988 - val_accuracy: 0.9360\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 460s 588ms/step - loss: 0.3902 - accuracy: 0.8266 - val_loss: 0.3338 - val_accuracy: 0.8800\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 516s 660ms/step - loss: 0.3288 - accuracy: 0.8623 - val_loss: 0.2482 - val_accuracy: 0.9040\n",
      "Accuracy: 90.40%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb #Загружаем датесет IMDb,\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "top_words = 5000\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=top_words)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "X_test = data[:250]\n",
    "y_test = targets[:250]\n",
    "X_train = data[1:]\n",
    "y_train = targets[1:]\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32 # длина вектора, в котором будут представляться слова\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length)) # плотный вектор\n",
    "# input_length - размер входных данных\n",
    "model.add(LSTM(100)) # рекуррентный слой\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c689d8de-49c8-4ffd-9854-24f4adaa8fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "782/782 [==============================] - 342s 432ms/step - loss: 0.3497 - accuracy: 0.8353 - val_loss: 0.2407 - val_accuracy: 0.9259\n",
      "Epoch 2/3\n",
      "782/782 [==============================] - 264s 338ms/step - loss: 0.2276 - accuracy: 0.9115 - val_loss: 0.1410 - val_accuracy: 0.9593\n",
      "Epoch 3/3\n",
      "782/782 [==============================] - 245s 313ms/step - loss: 0.1964 - accuracy: 0.9254 - val_loss: 0.1329 - val_accuracy: 0.9556\n",
      "Accuracy: 95.56%\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb #Загружаем датесет IMDb,\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.datasets import imdb\n",
    "\n",
    "top_words = 5000\n",
    "(training_data, training_targets), (testing_data, testing_targets) = imdb.load_data(num_words=top_words)\n",
    "data = np.concatenate((training_data, testing_data), axis=0)\n",
    "targets = np.concatenate((training_targets, testing_targets), axis=0)\n",
    "\n",
    "X_test = data[:270] \n",
    "y_test = targets[:270]\n",
    "X_train = data[1:]\n",
    "y_train = targets[1:]\n",
    "\n",
    "max_review_length = 500\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "\n",
    "embedding_vecor_length = 32 #на вход получает номера слов, а на выходе выдаёт их векторные представления \n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')) # сверточный слой\n",
    "model.add(MaxPooling1D(pool_size=2)) # субдискретизирующий слой\n",
    "model.add(LSTM(100)) # рекуррентный слой\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=64)\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bb70597-5c46-4906-ae51-934b35638228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " This film is very well shot. I liked the acting. In general, the film is good.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POSITIVE [[0.9375609]]'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = imdb.get_word_index()\n",
    "def predict(txt:str):\n",
    "    txt = txt.lower()\n",
    "    txt1 = \"\"\n",
    "    for i in txt:\n",
    "        if('a'<=i<='z' or i==' '):\n",
    "            txt1+=i\n",
    "    txt1=txt1.split()\n",
    "    tokens=np.array([min(index.get(i, 5000),5000)+3 for i in txt1])\n",
    "    vector = sequence.pad_sequences([tokens], maxlen=max_review_length)\n",
    "    p=model.predict(vector)\n",
    "    return \"POSITIVE \"+str(p) if p>0.5 else \"NEGATIVE \"+str(1-p)\n",
    "\n",
    "predict(input())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacec871-9244-4795-80c3-7d10858824d0",
   "metadata": {},
   "source": [
    "## POSITIVE\n",
    "This film is very well shot. I liked the acting. In general, the film is good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b87d638-f44e-4467-9999-879c057832ef",
   "metadata": {},
   "source": [
    "## NEGATIVE\n",
    "I really wanted to watch this movie because of the actors who played in it. But after watching this movie, I was very disappointed. The plot seemed to me very uninteresting and boring. In general, we can say that I did not like the film."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796e5c96-e16d-48d0-8393-531646ec9366",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
