{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aace8cf-4db2-4787-a498-13fe1b58e57b",
   "metadata": {},
   "source": [
    "## Лабораторная работа №4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d253d94-d291-421f-b09a-882110919b36",
   "metadata": {},
   "source": [
    "## Распознавание рукописных символов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300c3e9c-b445-41a9-8205-07a8fab846e3",
   "metadata": {},
   "source": [
    "## Цель\n",
    "Реализовать классификацию черно-белых изображений рукописных цифр (28x28) по 10\n",
    "категориям (от 0 до 9)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc29f031-3d80-41d8-b993-e37dd04edae0",
   "metadata": {},
   "source": [
    "## Задачи\n",
    "1. Ознакомиться с представлением графических данных\n",
    "2. Ознакомиться с простейшим способом передачи графических данных нейронной сети\n",
    "3. Создать модель\n",
    "4.  Настроить параметры обучения\n",
    "5.  Написать функцию, позволяющая загружать изображение пользователи и классифицировать его"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c317c68-28d4-460a-ad5c-15d531a50d22",
   "metadata": {},
   "source": [
    "## Выполнение работы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652723c7-ee97-431a-bc19-b8bb25668b4c",
   "metadata": {},
   "source": [
    "Набор данных MNIST уже входит в состав Keras в форме набора из четырех массивов\n",
    "Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "969084d7-a62a-46ae-8f3c-6e147cdc054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(train_images, train_labels),(test_images, test_labels) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15397ae1-8217-4203-b158-07e040377636",
   "metadata": {},
   "source": [
    "Здесь train_images и train_labels — это тренировочный набор, то есть данные,\n",
    "необходимые для обучения. После обучения модель будет проверяться тестовым (или\n",
    "контрольным) набором, test_images и test_labels. Изображения хранятся в массивах\n",
    "Numpy, а метки — в массиве цифр от 0 до 9. Изображения и метки находятся в прямом\n",
    "соответствии, один к одному.\n",
    "Для проверки корректности загрузки достаточно сравнить тестовое изображение с его\n",
    "меткой."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7054758-f57c-4d8c-a0ad-58b52e3a6424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_images[1],cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print(train_labels[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787fa78-1a9b-4ad1-af0f-53dea419679f",
   "metadata": {},
   "source": [
    "Исходные изображения представлены в виде массивов чисел в интервале [0, 255]. Перед\n",
    "обучением их необходимо преобразовать так, чтобы все значения оказались в интервале\n",
    "[0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7bd376dc-6e28-43c9-8794-3a69bc4b371a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images / 255.0\n",
    "test_images = test_images / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a20258-6243-48f0-b025-f9867ba392b7",
   "metadata": {},
   "source": [
    "Также необходимо закодировать метки категорий. В данном случае прямое кодирование\n",
    "меток заключается в конструировании вектора с нулевыми элементами со значением 1 в\n",
    "элементе, индекс которого соответствует индексу метки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0772e032-2411-4201-92b4-4809ad828a3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "print(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee73b175-f79b-4114-b3bd-463941fbf81d",
   "metadata": {},
   "source": [
    "Теперь можно задать базовую архитектуру сети."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf90a9d8-b76f-4f99-afc7-2cbea2a60bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.models import Sequential\n",
    "model = Sequential()\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c25f11-713b-4695-8148-5f22e61db84e",
   "metadata": {},
   "source": [
    "Чтобы подготовить сеть к обучению, нужно настроить еще три параметра для этапа\n",
    "компиляции:\n",
    "1. функцию потерь, которая определяет, как сеть должна оценивать качество своей\n",
    "работы на обучающих данных и, соответственно, как корректировать ее в\n",
    "правильном направлении;\n",
    "2. оптимизатор — механизм, с помощью которого сеть будет обновлять себя, опираясь\n",
    "на наблюдаемые данные и функцию потерь;\n",
    "3. метрики для мониторинга на этапах обучения и тестирования — здесь нас будет\n",
    "интересовать только точность (доля правильно классифицированных\n",
    "изображений)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7be6406-7dbf-44c2-8b89-37e161094d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82753f07-5877-49d6-aef1-defe58dee93d",
   "metadata": {},
   "source": [
    "Теперь можно начинать обучение сети, для чего в случае использования библиотеки Keras\n",
    "достаточно вызвать метод fit сети — он пытается адаптировать (fit) модель под обучающие\n",
    "данные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08ccb715-a533-4873-841f-6d8016ddb0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3043 - accuracy: 0.9156\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1320 - accuracy: 0.9620\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0898 - accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0671 - accuracy: 0.9806\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0519 - accuracy: 0.9850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x215f27386d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_images, train_labels, epochs=5, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9eecf4-35d8-463a-8e57-1427d5aeaa2a",
   "metadata": {},
   "source": [
    "В процессе обучения отображаются две величины: потери сети на обучающих данных и\n",
    "точность сети на обучающих данных.\n",
    "Теперь проверим, как модель распознает контрольный набор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f4ab063-aa9a-453c-bc68-76987b875870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0737 - accuracy: 0.9756\n",
      "test_acc: 0.975600004196167\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671ddd4c-9e81-4b2f-96bb-fbf96d47919a",
   "metadata": {},
   "source": [
    "## Требования\n",
    "\n",
    "1. Найти архитектуру сети, при которой точность классификации будет не менее 95%\n",
    "2. Исследовать влияние различных оптимизаторов, а также их параметров, на процесс обучения\n",
    "3. Написать функцию, которая позволит загружать пользовательское изображение не из датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc934858-9a43-450e-a66b-3462a2f763bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1 Найдем архитектуру, при которой точность классификации будет не менее 95 процентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdfc36ef-d5db-4d40-a71f-5fd1e8f36e9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3745 - accuracy: 0.8978\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1772 - accuracy: 0.9499\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1288 - accuracy: 0.9631\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1020 - accuracy: 0.9705\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0824 - accuracy: 0.9764\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0953 - accuracy: 0.9717\n",
      "test_acc: 0.9717000126838684\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Flatten())\n",
    "model2.add(Dense(100, activation='relu'))\n",
    "model2.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model2.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model2.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model2.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05fedfdc-f2b3-4b78-96a5-ee27e10b4469",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3102 - accuracy: 0.9129\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1221 - accuracy: 0.9640\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0842 - accuracy: 0.9743\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0632 - accuracy: 0.9807\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0502 - accuracy: 0.9846\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0863 - accuracy: 0.9743\n",
      "test_acc: 0.9743000268936157\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model3.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model3.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model3.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d195322-bc2c-4f59-bd94-b02c2847211a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2117 - accuracy: 0.9349\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0925 - accuracy: 0.9711\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0654 - accuracy: 0.9787\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0507 - accuracy: 0.9843\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0397 - accuracy: 0.9872\n",
      "313/313 [==============================] - 1s 1ms/step - loss: 0.0803 - accuracy: 0.9779\n",
      "test_acc: 0.9779000282287598\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Flatten())\n",
    "model4.add(Dense(128, activation='relu'))\n",
    "model4.add(Dense(784, activation='relu'))\n",
    "model4.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model4.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model4.fit(train_images, train_labels, epochs=5, batch_size=32)\n",
    "\n",
    "test_loss, test_acc = model4.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d740c0c9-75fa-4a2a-9fe8-4f371ae8a1df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.1791 - accuracy: 0.9447\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0818 - accuracy: 0.9751\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0582 - accuracy: 0.9816\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 17s 9ms/step - loss: 0.0459 - accuracy: 0.9855\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 16s 9ms/step - loss: 0.0363 - accuracy: 0.9887\n",
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0855 - accuracy: 0.9767\n",
      "test_acc: 0.9767000079154968\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Flatten())\n",
    "model5.add(Dense(784, activation='relu'))\n",
    "model5.add(Dense(1000, activation='relu'))\n",
    "model5.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model5.compile(optimizer='adam',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model5.fit(train_images, train_labels, epochs=5, batch_size=32)\n",
    "\n",
    "test_loss, test_acc = model5.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605340ee-267e-461a-b251-dfe71f73fd06",
   "metadata": {},
   "source": [
    "## Исследуем различные оптимизаторы и их параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b56edb16-2c24-4ab8-972c-6ea8f59fc6b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.2112 - accuracy: 0.9356\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.1082 - accuracy: 0.9673\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0883 - accuracy: 0.9740\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0768 - accuracy: 0.9768\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.0676 - accuracy: 0.9802\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.1425 - accuracy: 0.9679\n",
      "test_acc: 0.9678999781608582\n"
     ]
    }
   ],
   "source": [
    "model6 = Sequential()\n",
    "model6.add(Flatten())\n",
    "model6.add(Dense(256, activation='relu'))\n",
    "model6.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model6.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model6.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model6.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25630337-07fa-422b-af6e-2dbc1fc128c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 2s 3ms/step - loss: 0.2898 - accuracy: 0.9182\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.1270 - accuracy: 0.9635\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0860 - accuracy: 0.9754\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0644 - accuracy: 0.9809\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 3ms/step - loss: 0.0500 - accuracy: 0.9848\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.0784 - accuracy: 0.9761\n",
      "test_acc: 0.9761000275611877\n"
     ]
    }
   ],
   "source": [
    "model7 = Sequential()\n",
    "model7.add(Flatten())\n",
    "model7.add(Dense(256, activation='relu'))\n",
    "model7.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model7.compile(optimizer='RMSprop',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model7.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model7.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8f44b2b0-590a-4155-9250-02eb45a039b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 1.1259 - accuracy: 0.7381\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.5380 - accuracy: 0.8680\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.4315 - accuracy: 0.8861\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3837 - accuracy: 0.8959\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 1s 2ms/step - loss: 0.3547 - accuracy: 0.9027\n",
      "313/313 [==============================] - 0s 1ms/step - loss: 0.3264 - accuracy: 0.9103\n",
      "test_acc: 0.9103000164031982\n"
     ]
    }
   ],
   "source": [
    "model8 = Sequential()\n",
    "model8.add(Flatten())\n",
    "model8.add(Dense(256, activation='relu'))\n",
    "model8.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model8.compile(optimizer='SGD',loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model8.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
    "\n",
    "test_loss, test_acc = model8.evaluate(test_images, test_labels)\n",
    "print('test_acc:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d12e8c-f7ac-4f57-a8ac-c429d791f6ac",
   "metadata": {},
   "source": [
    "## Функция которая позволяет загружать пользовательские изображения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf494063-7e9c-4217-9333-8669b0feb6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "def img(name_img):\n",
    "    img_new = Image.open(name_img)\n",
    "    img_new = img_new.convert('L')\n",
    "    img_new = np.asarray(img_new)\n",
    "    img_new = img_new / 255.0\n",
    "    x = np.expand_dims(img_new, axis=0)\n",
    "    res = model4.predict(x)\n",
    "    print(res)\n",
    "    print(f\"Распознанная цифра: {np.argmax(res)}\")\n",
    "    plt.imshow(img_new, cmap=plt.cm.binary)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "163df37d-6085-4eaa-9387-48c0cf49da1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.1683397e-04 6.8079608e-08 1.0695386e-06 2.4221831e-06 2.5376627e-05\n",
      "  9.0215954e-06 9.9943382e-01 7.5267267e-06 3.7167761e-06 1.3366969e-07]]\n",
      "Распознанная цифра: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img('first_img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "757da76c-9d97-4e8a-bfe0-836a879adae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.1451775e-09 1.7779352e-09 1.5382589e-08 1.5548991e-04 9.4840036e-10\n",
      "  2.4359794e-08 8.3587460e-12 8.4481880e-11 9.9984431e-01 8.6592564e-08]]\n",
      "Распознанная цифра: 8\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img('second_img.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075b0e24-d012-419d-a2a1-9173d72144d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
